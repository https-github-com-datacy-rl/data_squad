{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff5e3be7-9d68-4896-ae33-45ab480417dc",
   "metadata": {},
   "source": [
    "# Decision Trees and Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358f6e81-eed4-44fb-92c3-c31f021c1fe0",
   "metadata": {},
   "source": [
    "Decision Trees and Random Forests are powerful algorithms used for both classification and regression tasks. They are part of the family of ensemble methods, which are techniques that combine multiple models to produce one optimal predictive model.\n",
    "\n",
    "Decision Trees\n",
    "Decision Trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Structure: A decision tree is represented as a binary tree. It consists of nodes, edges, and leaves. Each internal node denotes a test on an attribute, each branch represents the outcome of the test, and each leaf node holds a class label.\n",
    "\n",
    "Building the Tree:\n",
    "\n",
    "The dataset is split based on the feature values. Splits are made by choosing the split that results in the highest purity of the nodes.\n",
    "Common metrics to measure purity/impurity are Gini Impurity, Entropy, and Classification Error for classification tasks, and Variance Reduction for regression.\n",
    "Pruning: This is the process of removing parts of the tree that do not provide power to classify instances. Pruning reduces the complexity of the final classifier, which helps in reducing overfitting.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "Easy to understand and interpret.\n",
    "Requires little data preparation.\n",
    "Disadvantages:\n",
    "\n",
    "Can create overly complex trees that do not generalize the data well (overfitting).\n",
    "Decision trees can be unstable because small variations in the data might result in a completely different tree.\n",
    "Random Forests\n",
    "Random Forests are an ensemble learning method that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.\n",
    "\n",
    "Key Points:\n",
    "\n",
    "Ensemble Method: Random forests combine multiple decision trees to produce a more robust, accurate model.\n",
    "\n",
    "Building the Forest:\n",
    "\n",
    "Each tree in the forest is built from a sample drawn with replacement (i.e., a bootstrap sample) from the training set.\n",
    "When splitting a node during the construction of the tree, the best split is found either from all input features or a random subset of size max_features.\n",
    "Advantages:\n",
    "\n",
    "Reduction in overfitting: by averaging multiple trees, there is a lower risk of overfitting than with individual decision trees.\n",
    "Great with high-dimensional spaces and a large number of training examples.\n",
    "Disadvantages:\n",
    "\n",
    "More complex and computationally intensive than individual decision trees.\n",
    "Less intuitive: the final model can be difficult to interpret compared to a single decision tree.\n",
    "Differences between Decision Trees and Random Forests\n",
    "Complexity: Decision Trees are simpler and more interpretable. Random Forests are more complex and consist of multiple decision trees.\n",
    "Performance: Random Forests generally provide better predictive accuracy than Decision Trees due to averaging the results of multiple trees.\n",
    "Overfitting: Decision Trees are prone to overfitting, especially with noisy data. Random Forests tend to generalize better, reducing the risk of overfitting.\n",
    "Both Decision Trees and Random Forests are popular due to their versatility, interpretability, and good performance across a variety of problems. They are commonly used in classification tasks but can also be applied to regression. Understanding the nuances of each can help in choosing the right algorithm for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dd9ae7-2265-44fe-ac0b-e2294b2b47c1",
   "metadata": {},
   "source": [
    "Support Vector Machines (SVM) are a set of supervised learning methods used for classification, regression, and outliers detection. They are particularly well-suited for classification of complex but small- or medium-sized datasets.\n",
    "\n",
    "Key Concepts of SVM\n",
    "Hyperplane: SVM works by finding the hyperplane that best separates the classes in the feature space. In two dimensions, this hyperplane is a line.\n",
    "\n",
    "Support Vectors: Support vectors are the data points nearest to the hyperplane, and they influence the position and orientation of the hyperplane. These points are on the margin of the hyperplane.\n",
    "\n",
    "Margin: It's the gap between the two lines on the closest class points. A good margin is one where this separation is as wide as possible.\n",
    "\n",
    "Kernel Trick: Sometimes, the dataset can't be separated linearly. The kernel trick involves transforming the data into a higher dimension where a hyperplane can be used to separate the data.\n",
    "\n",
    "Types of SVM\n",
    "Linear SVM: Used for linearly separable data, which means that if data points of different classes can be separated by a single line.\n",
    "\n",
    "Non-linear SVM: Used for non-linearly separable data, which means that if data points of different classes cannot be separated by a single line. This is where the kernel trick comes into play.\n",
    "\n",
    "The SVM Algorithm\n",
    "Training:\n",
    "\n",
    "The algorithm builds a model that assigns new data points to one category or the other by finding a hyperplane that separates the classes in the feature space.\n",
    "The best hyperplane is the one that represents the largest separation, or margin, between the two classes.\n",
    "If the data is not linearly separable, SVM finds a soft margin hyperplane by allowing some misclassifications. Kernel functions can be used to solve this problem.\n",
    "Prediction:\n",
    "\n",
    "Once the model is trained, predictions are made using the hyperplane. New data is mapped into the same space and predicted to belong to a category based on which side of the hyperplane they fall on.\n",
    "Kernel Functions\n",
    "The kernel function transforms the input data into a higher-dimensional space where a hyperplane can be used to separate classes. Common kernels include:\n",
    "\n",
    "Linear Kernel: No transformation is done, suitable for linearly separable data.\n",
    "Polynomial Kernel: Transforms data into a specified degree of polynomial.\n",
    "Radial Basis Function (RBF) Kernel or Gaussian Kernel: Useful for non-linear data. It transforms data into a higher dimension where it is linearly separable.\n",
    "Advantages of SVM\n",
    "Effective in High Dimensional Spaces: Especially effective when the number of dimensions is greater than the number of samples.\n",
    "Memory Efficient: Uses a subset of training points (support vectors), so it's also memory efficient.\n",
    "Versatile: Different kernel functions can be specified for the decision function. Common kernels are provided, but it's also possible to specify custom kernels.\n",
    "Disadvantages of SVM\n",
    "Algorithm Complexity: Not suitable for large datasets because of its high training time.\n",
    "Selection of a Suitable Kernel: Choosing the right kernel (and its parameters) can be complex and requires experience.\n",
    "No Probabilistic Explanation: SVM doesnâ€™t directly provide probability estimates, these are calculated using an expensive five-fold cross-validation.\n",
    "SVMs are a powerful tool, especially for small datasets with a high number of features. They are widely used in applications like image recognition, text classification, and bioinformatics. Understanding the theory and practical applications of SVM is essential for any machine learning practitioner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:data_eng]",
   "language": "python",
   "name": "conda-env-data_eng-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
